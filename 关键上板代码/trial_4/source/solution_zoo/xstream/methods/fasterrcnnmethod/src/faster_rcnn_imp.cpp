//
// Created by yaoyao.sun on 2019-04-23.
// Copyright (c) 2019 Horizon Robotics. All rights reserved.
//

#include <assert.h>
#include <stdint.h>

#include <algorithm>
#include <fstream>
#include <map>
#include <memory>
#include <mutex>
#include <set>
#include <string>
#include <utility>
#include <vector>
#include <unordered_map>

#include "bpu_predict/bpu_parse_utils.h"
#include "bpu_predict/bpu_parse_utils_extension.h"
#include "bpu_predict/bpu_predict.h"
#include "hobotlog/hobotlog.hpp"
#include "hobotxsdk/xstream_data.h"
#include "hobotxstream/profiler.h"
#include "horizon/vision_type/vision_type.hpp"
#include "json/json.h"
#include "opencv2/imgproc.hpp"
#include "FasterRCNNMethod/FasterRCNNMethod.h"
#include "FasterRCNNMethod/config.h"
#include "FasterRCNNMethod/faster_rcnn_imp.h"
#include "FasterRCNNMethod/result.h"
#include "FasterRCNNMethod/util.h"
#include "FasterRCNNMethod/yuv_utils.h"
#include "common/common.h"

#ifdef X3
#include "./bpu_predict_x3.h"
#endif

#define DMA_ALIGN_SIZE 64
#define BPU_CEIL_ALIGN(len) \
  ((((len) + DMA_ALIGN_SIZE - 1) / DMA_ALIGN_SIZE) * DMA_ALIGN_SIZE)

using hobot::vision::Attribute;
using hobot::vision::BBox;
using hobot::vision::CVImageFrame;
using hobot::vision::Landmarks;
using hobot::vision::Point;
using hobot::vision::Pose3D;
using hobot::vision::PymImageFrame;
using hobot::vision::Segmentation;

using xstream::BaseDataPtr;
using xstream::BaseDataVector;
using xstream::FasterRCNNParam;
using xstream::InputParamPtr;
using xstream::XStreamData;

using ImageFramePtr = std::shared_ptr<hobot::vision::ImageFrame>;

namespace faster_rcnn_method {
// used to decide output info for each layer of model. first we should use hbcc
// interface to get model's info,
// and get each layer's output info, then config it in config file.
static std::map<std::string, FasterRCNNBranchOutType> str2faster_rcnn_out_type =
    {{"bbox", FasterRCNNBranchOutType::BBOX},
     {"kps", FasterRCNNBranchOutType::KPS},
     {"mask", FasterRCNNBranchOutType::MASK},
     {"reid", FasterRCNNBranchOutType::REID},
     {"lmks2_label", FasterRCNNBranchOutType::LMKS2_LABEL},
     {"lmks2_offset", FasterRCNNBranchOutType::LMKS2_OFFSET},
     {"lmks1", FasterRCNNBranchOutType::LMKS1},
     {"3d_pose", FasterRCNNBranchOutType::POSE_3D},
     {"plate_color", FasterRCNNBranchOutType::PLATE_COLOR},
     {"plate_row", FasterRCNNBranchOutType::PLATE_ROW},
     {"kps_label", FasterRCNNBranchOutType::KPS_LABEL},
     {"kps_offset", FasterRCNNBranchOutType::KPS_OFFSET},
     {"lmks", FasterRCNNBranchOutType::LMKS}};

void FasterRCNNImp::ParseConfig(const std::string &config_file) {
  FR_Config cfg_jv;
  std::ifstream infile(config_file.c_str());
  infile >> cfg_jv;
  config_.reset(new Config(cfg_jv));

  core_id_ = config_->GetIntValue("core_id", 2);
  auto net_info = config_->GetSubConfig("net_info");
  model_name_ = net_info->GetSTDStringValue("model_name");
  model_version_ = net_info->GetSTDStringValue("model_version", "1.0.0");
  std::vector<std::shared_ptr<Config>> model_out_sequence =
      net_info->GetSubConfigArray("model_out_sequence");

  LOGD << "rcnn branch out type:";
  for (size_t i = 0; i < model_out_sequence.size(); ++i) {
    std::string type_str = model_out_sequence[i]->GetSTDStringValue("type");
    HOBOT_CHECK(!type_str.empty());
    LOGD << type_str;
    if (str2faster_rcnn_out_type.find(type_str) ==
        str2faster_rcnn_out_type.end()) {
      out_level2rcnn_branch_info_[i].type = FasterRCNNBranchOutType::INVALID;
    } else {
      out_level2rcnn_branch_info_[i].type = str2faster_rcnn_out_type[type_str];
      out_level2rcnn_branch_info_[i].name =
          model_out_sequence[i]->GetSTDStringValue("name");
      out_level2rcnn_branch_info_[i].box_name =
          model_out_sequence[i]->GetSTDStringValue("box_name");
      out_level2rcnn_branch_info_[i].labels =
          model_out_sequence[i]->GetLabelsMap("labels");
    }
  }

  model_input_width_ = net_info->GetIntValue("model_input_width", 960);
  model_input_height_ = net_info->GetIntValue("model_input_height", 540);
  pyramid_layer_ = net_info->GetIntValue("pyramid_layer", 4);

  resize_input_width_ = model_input_width_;
  resize_input_height_ = model_input_height_;

  kps_pos_distance_ = net_info->GetFloatValue("kps_pos_distance", 0.1);
  kps_feat_width_ = net_info->GetIntValue("kps_feat_width", 16);
  kps_feat_height_ = net_info->GetIntValue("kps_feat_height", 16);
  kps_points_number_ = net_info->GetIntValue("kps_points_number", 17);
  kps_feat_stride_ = net_info->GetIntValue("kps_feat_stride", 16);
  kps_anchor_param_ = net_info->GetFloatValue("kps_anchor_param", 0.0);

  lmk_feat_height_ = net_info->GetIntValue("lmk_feat_height", 8);
  lmk_feat_width_ = net_info->GetIntValue("lmk_feat_width", 8);
  lmk_feat_stride_ = net_info->GetIntValue("lmk_feat_stride", 16);
  lmk_points_number_ = net_info->GetIntValue("lmk_points_number", 5);
  lmk_pos_distance_ = net_info->GetFloatValue("lmk_pos_distance", 12);
  lmk_anchor_param_ = net_info->GetFloatValue("lmk_anchor_param", 0.0);
  face_pose_number_ = net_info->GetIntValue("3d_pose_number", 3);

  plate_color_num_ = net_info->GetIntValue("plate_color_num", 6);
  plate_row_num_ = net_info->GetIntValue("plate_row_num", 2);

  face_pv_thr_ = net_info->GetFloatValue("face_pv_thr", 0.0);

  method_outs_ = config_->GetSTDStringArray("method_outs");
  LOGD << "method out type:";
  for (const auto &method_out : method_outs_) {
    LOGD << method_out;
  }

  is_crop_ = config_->GetBoolValue("is_crop", false);

  std::string parent_path = GetParentPath(config_file);
  bpu_config_path_ =
      parent_path + config_->GetSTDStringValue("bpu_config_path");
  model_file_path_ =
      parent_path + config_->GetSTDStringValue("model_file_path");
  LOGD << "config file parent path: " << parent_path
       << " bpu_config_path: " << bpu_config_path_
       << " model_file_path: " << model_file_path_;
}

void FasterRCNNImp::GetModelInfo(const std::string &model_name) {
  uint32_t output_layer_num = bpu_model_->output_num;
  for (size_t i = 0; i < output_layer_num; ++i) {
    const auto &branch_info = out_level2rcnn_branch_info_[i];
    auto out_type = branch_info.type;
    // get shifts
    const uint8_t *shift_value = bpu_model_->outputs[i].shifts;
    LOGD << "out_type:" << static_cast<int>(out_type)
         << " shift_len:" << bpu_model_->outputs[i].shift_len;

    // dim of per shape = 4
    int aligned_shape_dim = bpu_model_->outputs[i].aligned_shape.ndim;
    HOBOT_CHECK(aligned_shape_dim == 4)
        << "aligned_shape_dim = " << aligned_shape_dim;
    int *aligned_dim = new int[aligned_shape_dim];
    for (int dim = 0; dim < aligned_shape_dim; dim++) {
      aligned_dim[dim] = bpu_model_->outputs[i].aligned_shape.d[dim];
    }
    switch (out_type) {
      case FasterRCNNBranchOutType::INVALID:
        break;
      case FasterRCNNBranchOutType::KPS:
        // TODO(yaoyao.sun) pack into a function
        for (int idx = 0; idx < bpu_model_->outputs[i].shift_len; idx++) {
               kps_shifts_.push_back(shift_value[idx]);
        }
        aligned_kps_dim = aligned_dim;
        break;
      case FasterRCNNBranchOutType::KPS_LABEL:
        // TODO(yaoyao.sun) pack into a function
        kps_label_shift_ = shift_value[0];
        aligned_kps_label_dim = aligned_dim;
        break;
      case FasterRCNNBranchOutType::KPS_OFFSET:
        // TODO(yaoyao.sun) pack into a function
        kps_offset_shift_ = shift_value[0];
        aligned_kps_offset_dim = aligned_dim;
        break;
      case FasterRCNNBranchOutType::MASK:
        mask_shift_ = shift_value[0];
        aligned_mask_dim = aligned_dim;
        break;
      case FasterRCNNBranchOutType::REID:
        reid_shift_ = shift_value[0];
        aligned_reid_dim = aligned_dim;
        break;
      case FasterRCNNBranchOutType::LMKS:
        lmks_shift_ = shift_value[0];
        aligned_lmks_dim = aligned_dim;
        break;
      case FasterRCNNBranchOutType::LMKS2_LABEL:
        lmks2_label_shift_ = shift_value[0];
        aligned_lmks2_label_dim = aligned_dim;
        break;
      case FasterRCNNBranchOutType::LMKS2_OFFSET:
        lmks2_offset_shift_ = shift_value[0];
        aligned_lmks2_offset_dim = aligned_dim;
        break;
      case FasterRCNNBranchOutType::LMKS1:
        lmks1_shift_ = shift_value[0];
        aligned_lmks1_dim = aligned_dim;
        break;
      case FasterRCNNBranchOutType::POSE_3D:
        face_pose_shift_ = shift_value[0];
        aligned_face_pose_dim = aligned_dim;
        break;
      case FasterRCNNBranchOutType::PLATE_COLOR:
        plate_color_shift_ = shift_value[0];
        aligned_plate_color_dim = aligned_dim;
        HOBOT_CHECK(plate_color_num_ <= aligned_plate_color_dim[3]);
        break;
      case FasterRCNNBranchOutType::PLATE_ROW:
        plate_row_shift_ = shift_value[0];
        aligned_plate_row_dim = aligned_dim;
        HOBOT_CHECK(plate_row_num_ <= aligned_plate_row_dim[3]);
        break;
      default:
        break;
    }
  }
}

int FasterRCNNImp::Init(const std::string &config_file) {
  faster_rcnn_param_ = nullptr;
  // parse config file.
  ParseConfig(config_file);

  int ret = 0;
  // load model
  {
    std::ifstream ifs(model_file_path_.c_str(),
                      std::ios::in | std::ios::binary);
    if (!ifs) {
      HOBOT_CHECK(0) << "Open model file: " << model_file_path_ << " failed";
    }
    ifs.seekg(0, std::ios::end);
    int model_length = ifs.tellg();
    ifs.seekg(0, std::ios::beg);
    char *model_bin = new char[sizeof(char) * model_length];
    ifs.read(model_bin, model_length);
    ifs.close();
    bpu_model_ = new BPU_MODEL_S();
    ret = HB_BPU_loadModel(model_bin, model_length, bpu_model_);
    HOBOT_CHECK(ret == 0) << "load model failed" << HB_BPU_getErrorName(ret);
    delete[] model_bin;
  }

  LOGD << "BPU_getModelOutputInfo success.";
  GetModelInfo(model_name_);
  return 0;
}


void FasterRCNNImp::RunSingleFrameWithinCrop(
    const std::vector<BaseDataPtr> &frame_input,
    std::vector<BaseDataPtr> &frame_output) {
  HOBOT_CHECK(frame_input.size() == 2)
  << "please insure that frame_input[0] is the image frame, "
  << "frame_input[1] is the vector of body boxes";

  RUN_PROCESS_TIME_PROFILER("FasterRCNN RunSingleFrameWithinCrop");
  RUN_FPS_PROFILER("FasterRCNN RunSingleFrameWithinCrop");

  const auto frame_img_ = frame_input[0];
  for (size_t out_index = 0; out_index < method_outs_.size(); ++out_index) {
    frame_output.push_back(std::make_shared<xstream::BaseDataVector>());
  }

  auto xstream_img =
      std::static_pointer_cast<XStreamData<ImageFramePtr>>(frame_img_);

  auto xstream_rois = std::static_pointer_cast<BaseDataVector>(frame_input[1]);
  size_t rois_num = xstream_rois->datas_.size();

  std::string img_type = xstream_img->value->type;
  LOGD << "image type: " << img_type << std::endl;

  for (size_t roi_idx = 0; roi_idx < rois_num; roi_idx++) {
    HobotXStreamImageToolsResizeInfo resize_info;
    int src_img_height = 0, src_img_width = 0;
    int padding_right = 0, padding_bottom = 0;
    float scale = 0;

    // 1. prepare data for croping
    std::shared_ptr<XStreamData<hobot::vision::BBox>> p_roi;
    p_roi = std::static_pointer_cast<XStreamData<hobot::vision::BBox>>(
        xstream_rois->datas_[roi_idx]);
    hobot::vision::BBox *norm_box = &(p_roi->value);

    if (p_roi->state_ != xstream::DataState::VALID) {
      LOGI << "BBox is not valid";
      continue;
    }

    if (img_type == kPyramidImage) {
      auto pyramid_image =
          std::static_pointer_cast<PymImageFrame>(xstream_img->value);
      uint8_t *nv12_data;
      // 2. crop and resize Roi
      CropPadAndResizeRoi(norm_box, pyramid_image,
          &nv12_data, &resize_info, &scale);
      padding_right =
          resize_info.padding_right_ / resize_info.width_ratio_;
      padding_right &= ~1;
      padding_bottom =
          resize_info.padding_bottom_ / resize_info.height_ratio_;
      padding_bottom &= ~1;

      src_img_height = resize_info.src_height_ + padding_bottom;
      src_img_width = resize_info.src_width_ + padding_right;

      int img_len = src_img_width * src_img_height * 3 / 2;
      RunModel(nv12_data, img_len, src_img_width,
               src_img_height, BPU_TYPE_IMG_NV12_SEPARATE);
      HobotXStreamFreeImage(nv12_data);
    } else if (img_type == kCVImageFrame) {
      // TODO(zhy)
    } else {
      HOBOT_CHECK(0) << "Do not support such img_type: " << img_type;
    }
    {
      RUN_PROCESS_TIME_PROFILER("FasterRCNN SinglePerson PostProcess");
      RUN_FPS_PROFILER("FasterRCNN SinglePerson PostProcess");

      // Post process
      FlushOutputTensor();
      GetFrameOutput(src_img_width * scale, src_img_height * scale,
                     frame_output, norm_box);
    }
  }
}

void FasterRCNNImp::RunSingleFrame(const std::vector<BaseDataPtr> &frame_input,
                                   std::vector<BaseDataPtr> &frame_output) {
  if (is_crop_) {
    RunSingleFrameWithinCrop(frame_input, frame_output);
  } else {
    // only one input slot -> PyramidImage or CVImage
    HOBOT_CHECK(frame_input.size() == 1);
    const auto frame_img_ = frame_input[0];

    for (size_t out_index = 0; out_index < method_outs_.size(); ++out_index) {
      frame_output.push_back(std::make_shared<xstream::BaseDataVector>());
    }

    auto xstream_img =
        std::static_pointer_cast<XStreamData<ImageFramePtr>>(frame_img_);

    std::string img_type = xstream_img->value->type;
    LOGD << "image type: " << img_type << std::endl;

    int ret = 0;

    int src_img_width = 0;
    int src_img_height = 0;
    int target_pym_layer_width = 0;
    int target_pym_layer_height = 0;

    {
      RUN_PROCESS_TIME_PROFILER("FasterRCNN RunModelFromPyramid");
      RUN_FPS_PROFILER("FasterRCNN RunModelFromPyramid");

      if (img_type == kPyramidImage) {
        auto pyramid_image =
            std::static_pointer_cast<PymImageFrame>(xstream_img->value);

#ifdef X2
        src_img_height = pyramid_image->img.src_img.height;
        src_img_width = pyramid_image->img.src_img.width;
        target_pym_layer_height =
            pyramid_image->img.down_scale[pyramid_layer_].height;
        target_pym_layer_width =
            pyramid_image->img.down_scale[pyramid_layer_].width;
#endif

#ifdef X3
        src_img_height = pyramid_image->down_scale[0].height;
      src_img_width = pyramid_image->down_scale[0].width;
      target_pym_layer_height =
        pyramid_image->down_scale[pyramid_layer_].height;
      target_pym_layer_width =
        pyramid_image->down_scale[pyramid_layer_].width;
#endif

        resize_input_width_ = target_pym_layer_width;
        resize_input_height_ = target_pym_layer_height;

        LOGD << "img_height: " << pyramid_image->Height()
             << ", img_width: " << pyramid_image->Width()
             << ", img_y length: " << pyramid_image->DataSize()
             << ", img_uv length: " << pyramid_image->DataUVSize();

        if (model_input_width_ != target_pym_layer_width ||
            model_input_height_ != target_pym_layer_height) {
          // get pym level 4 (960*540), then pad to 960*544
          {
            RUN_PROCESS_TIME_PROFILER("FasterRCNN PaddingImage");
            uint8_t *pOutputImg = nullptr;
            int output_img_size = 0;
            int output_img_width = 0;
            int output_img_height = 0;
            int first_stride = 0;
            int second_stride = 0;
#ifdef X2
            auto input_img = pyramid_image->img.down_scale[pyramid_layer_];
            int y_size = input_img.height * input_img.step;
            int uv_size = y_size >> 1;
            const uint8_t *input_nv12_data[3] =
                {reinterpret_cast<uint8_t *>(input_img.y_vaddr),
                 reinterpret_cast<uint8_t *>(input_img.c_vaddr), nullptr};
            const int input_nv12_size[3] = {y_size, uv_size, 0};
            ret = HobotXStreamCropYuvImageWithPaddingBlack(
                input_nv12_data, input_nv12_size,
                input_img.width, input_img.height,
                input_img.step, input_img.step, IMAGE_TOOLS_RAW_YUV_NV12,
                0, 0, model_input_width_ - 1, model_input_height_ - 1,
                &pOutputImg, &output_img_size,
                &output_img_width, &output_img_height,
                &first_stride, &second_stride);
#endif

#ifdef X3
            auto input_img = pyramid_image->down_scale[pyramid_layer_];
          int y_size = input_img.height * input_img.stride;
          int uv_size = y_size >> 1;
          const uint8_t *input_nv12_data[3] =
            {reinterpret_cast<uint8_t *>(input_img.y_vaddr),
             reinterpret_cast<uint8_t *>(input_img.c_vaddr), nullptr};
          const int input_nv12_size[3] = {y_size, uv_size, 0};
          ret = HobotXStreamCropYuvImageWithPaddingBlack(
            input_nv12_data, input_nv12_size, input_img.width, input_img.height,
            input_img.stride, input_img.stride, IMAGE_TOOLS_RAW_YUV_NV12,
            0, 0, model_input_width_ - 1, model_input_height_ - 1,
            &pOutputImg, &output_img_size,
            &output_img_width, &output_img_height,
            &first_stride, &second_stride);
#endif
            if (ret < 0) {
              LOGE << "fail to crop image";
              free(pOutputImg);
              return;
            }
            HOBOT_CHECK(output_img_width == model_input_width_)
            << "cropped image width "<< output_img_width
            << " not equals to model input width " << model_input_width_;
            HOBOT_CHECK(output_img_height == model_input_height_)
            << "cropped image height " << output_img_height
            << " not equals to model input height " << model_input_height_;
            int img_len = model_input_width_ * model_input_height_ * 3 / 2;
            LOGD << "Begin call RunModel";
            ret = RunModel(pOutputImg, img_len, BPU_TYPE_IMG_NV12_SEPARATE);
            if (ret != 0) {
              LOGE << "Run model failed: " << HB_BPU_getErrorName(ret);
              HobotXStreamFreeImage(pOutputImg);
              return;
            }
            HobotXStreamFreeImage(pOutputImg);
          }
        } else {
          LOGD << "Begin call RunModel";
          ret = RunModelFromPym(pyramid_image.get(), pyramid_layer_,
                                BPU_TYPE_IMG_NV12_SEPARATE);
          if (ret != 0) {
            LOGE << "Run model failed: " << HB_BPU_getErrorName(ret);
            // ReleaseOutputTensor();
            return;
          }
        }
      } else if (img_type == kCVImageFrame) {
        auto cv_image =
            std::static_pointer_cast<CVImageFrame>(xstream_img->value);

        HOBOT_CHECK(cv_image->pixel_format ==
            HorizonVisionPixelFormat::kHorizonVisionPixelFormatRawBGR);

        src_img_height = cv_image->Height();
        src_img_width = cv_image->Width();
        LOGD << "image height: " << src_img_height
             << "width: " << src_img_width;

        auto img_mat = cv_image->img;
        cv::Mat resized_mat = img_mat;
        if (src_img_height != model_input_height_ &&
            src_img_width != model_input_width_) {
          LOGD << "need resize.";
          cv::resize(img_mat, resized_mat,
                     cv::Size(model_input_width_, model_input_height_));
        }

        resize_input_width_ = model_input_width_;
        resize_input_height_ = model_input_height_;

        cv::Mat img_nv12;
        uint8_t *bgr_data = resized_mat.ptr<uint8_t>();
        bgr_to_nv12(bgr_data, model_input_height_, model_input_width_,
            img_nv12);
        uint8_t *nv12_data = img_nv12.ptr<uint8_t>();

        int img_len = model_input_width_ * model_input_height_ * 3 / 2;
        LOGD << "nv12 img_len: " << img_len;

        LOGD << "Begin call RunModel";
        ret = RunModel(nv12_data, img_len, BPU_TYPE_IMG_NV12_SEPARATE);
        if (ret != 0) {
          LOGE << "Run model failed: " << HB_BPU_getErrorName(ret);
          // ReleaseOutputTensor();
          return;
        }
      } else {
        HOBOT_CHECK(0) << "Not support this input type: " << img_type;
      }
    }

    RUN_PROCESS_TIME_PROFILER("FasterRCNN PostProcess");
    RUN_FPS_PROFILER("FasterRCNN PostProcess");

    // Post process
    FlushOutputTensor();
    GetFrameOutput(src_img_width, src_img_height, frame_output);

    // release output tensor
    // ReleaseOutputTensor();
  }
}

void FasterRCNNImp::GetFrameOutput(int src_img_width, int src_img_height,
                                   std::vector<BaseDataPtr> &frame_output,
                                   BBox *bbox) {
  FasterRCNNOutMsg det_result;
  PostProcess(det_result);
  CoordinateTransform(det_result, src_img_width, src_img_height,
                      resize_input_width_, resize_input_height_,
                      bbox);
  for (auto &boxes : det_result.boxes) {
    LOGD << boxes.first << ", num: " << boxes.second.size();
    for (auto &box : boxes.second) {
      LOGD << box;
    }
  }

  // TODO(yaoyao.sun) Packaged into a function, GetResultMsg()
  // convert FasterRCNNOutMsg to xstream data structure
  std::map<std::string, std::shared_ptr<BaseDataVector>> xstream_det_result;

  // get landmark by landmark2 and landmark1.
  if (det_result.landmarks.find("landmark2") != det_result.landmarks.end() &&
      det_result.landmarks.find("landmark1") != det_result.landmarks.end()) {
    std::vector<Landmarks> vec_landmarks;
    auto &landmarks2 = det_result.landmarks["landmark2"];
    auto &landmarks1 = det_result.landmarks["landmark1"];
    HOBOT_CHECK(landmarks2.size() == landmarks1.size())
        << "landmarks2's size not equal to landmarks1's size.";
    for (size_t lmk_index = 0; lmk_index < landmarks2.size(); ++lmk_index) {
      Landmarks landmarks;
      auto &landmark2 = landmarks2[lmk_index];
      auto &landmark1 = landmarks1[lmk_index];
      HOBOT_CHECK(landmark2.values.size() == landmark1.values.size())
          << "landmark2's size not equal to landmark1's size.";
      for (size_t point_index = 0; point_index < landmark2.values.size();
           ++point_index) {
        auto &landmark2_point = landmark2.values[point_index];
        auto &landmark1_point = landmark1.values[point_index];
        if (landmark2_point.score > 0.5) {
          landmarks.values.push_back(std::move(landmark2_point));
        } else {
          landmarks.values.push_back(std::move(landmark1_point));
        }
      }
      vec_landmarks.push_back(landmarks);
    }
    det_result.landmarks["landmark"] = vec_landmarks;
    det_result.landmarks.erase("landmark2");
    det_result.landmarks.erase("landmark1");
  }

  std::set<int> invalid_idx;
  // box
  for (const auto &boxes : det_result.boxes) {
    xstream_det_result[boxes.first] =
        std::make_shared<xstream::BaseDataVector>();
    xstream_det_result[boxes.first]->name_ = "rcnn_" + boxes.first;
    for (uint i = 0; i < boxes.second.size(); ++i) {
      const auto &box = boxes.second[i];
      if (boxes.first == "face_box") {
        if (box.score < face_pv_thr_) {
          invalid_idx.emplace(i);
          continue;
        }
      }
      auto xstream_box = std::make_shared<XStreamData<BBox>>();
      xstream_box->value = std::move(box);
      xstream_det_result[boxes.first]->datas_.push_back(xstream_box);
    }
  }

  // landmark
  for (const auto &landmarks_vec : det_result.landmarks) {
    xstream_det_result[landmarks_vec.first] =
        std::make_shared<xstream::BaseDataVector>();
    xstream_det_result[landmarks_vec.first]->name_ =
        "rcnn_" + landmarks_vec.first;
    for (uint i = 0; i < landmarks_vec.second.size(); ++i) {
      if (invalid_idx.find(i) != invalid_idx.end()) {
        continue;
      }
      const auto &landmarks = landmarks_vec.second[i];
      LOGD << "lmk point: [";
      for (const auto &point : landmarks.values) {
        LOGD << point.x << "," << point.y << "," << point.score;
      }
      LOGD << "]";
      auto xstream_landmark = std::make_shared<XStreamData<Landmarks>>();
      xstream_landmark->value = std::move(landmarks);
      xstream_det_result[landmarks_vec.first]->datas_.push_back(
          xstream_landmark);
    }
  }

  // feature
  for (const auto &feature_vec : det_result.features) {
    xstream_det_result[feature_vec.first] =
        std::make_shared<xstream::BaseDataVector>();
    xstream_det_result[feature_vec.first]->name_ = "rcnn_" + feature_vec.first;
    for (auto &feature : feature_vec.second) {
      auto xstream_feature = std::make_shared<XStreamData<Feature>>();
      xstream_feature->value = std::move(feature);
      xstream_det_result[feature_vec.first]->datas_.push_back(xstream_feature);
    }
  }

  // segmentations
  for (const auto &segmentation_vec : det_result.segmentations) {
    xstream_det_result[segmentation_vec.first] =
        std::make_shared<xstream::BaseDataVector>();
    xstream_det_result[segmentation_vec.first]->name_ =
        "rcnn_" + segmentation_vec.first;
    for (auto &segmentation : segmentation_vec.second) {
      auto xstream_segmentation = std::make_shared<XStreamData<Segmentation>>();
      xstream_segmentation->value = std::move(segmentation);
      xstream_det_result[segmentation_vec.first]->datas_.push_back(
          xstream_segmentation);
    }
  }

  // poses
  for (const auto &pose_vec : det_result.poses) {
    xstream_det_result[pose_vec.first] =
        std::make_shared<xstream::BaseDataVector>();
    xstream_det_result[pose_vec.first]->name_ = "rcnn_" + pose_vec.first;
    for (uint i = 0; i < pose_vec.second.size(); ++i) {
      if (invalid_idx.find(i) != invalid_idx.end()) {
        continue;
      }
      const auto &pose = pose_vec.second[i];
      auto xstream_pose = std::make_shared<XStreamData<Pose3D>>();
      xstream_pose->value = std::move(pose);
      xstream_det_result[pose_vec.first]->datas_.push_back(xstream_pose);
    }
  }

  // attributes
  for (const auto &attribute_vec : det_result.attributes) {
    xstream_det_result[attribute_vec.first] =
        std::make_shared<xstream::BaseDataVector>();
    xstream_det_result[attribute_vec.first]->name_ =
        "rcnn_" + attribute_vec.first;
    for (auto &attribute : attribute_vec.second) {
      auto xstream_attribute = std::make_shared<XStreamData<Attribute<int>>>();
      xstream_attribute->value = std::move(attribute);
      xstream_det_result[attribute_vec.first]->datas_.push_back(
          xstream_attribute);
    }
  }

//  for (size_t out_index = 0; out_index < method_outs_.size(); ++out_index) {
//    if (xstream_det_result[method_outs_[out_index]]) {
//      frame_output[out_index] = xstream_det_result[method_outs_[out_index]];
//    }
//  }

  for (size_t out_index = 0; out_index < method_outs_.size(); ++out_index) {
    if (xstream_det_result[method_outs_[out_index]]) {
      auto frame_out_slot = std::static_pointer_cast<
          xstream::BaseDataVector>(frame_output[out_index]);
      auto det_result_slot = xstream_det_result[method_outs_[out_index]];
      frame_out_slot->datas_.insert(frame_out_slot->datas_.end(),
                                    det_result_slot->datas_.begin(),
                                    det_result_slot->datas_.end());
    }
  }
}

void FasterRCNNImp::PostProcess(FasterRCNNOutMsg &det_result) {
  void* lmk2_label_out_put = nullptr;
  void* lmk2_offset_out_put = nullptr;
  void* kps_label_out_put = nullptr;
  void* kps_offset_out_put = nullptr;

  for (int out_level = 0; out_level < bpu_model_->output_num; ++out_level) {
    const auto &branch_info = out_level2rcnn_branch_info_[out_level];
    auto out_type = branch_info.type;
    switch (out_type) {
      case FasterRCNNBranchOutType::INVALID:
        break;
      case FasterRCNNBranchOutType::BBOX:
        GetRppRects(det_result.boxes, out_level,
                    branch_info.name, branch_info.labels);
        break;
      case FasterRCNNBranchOutType::LMKS2_LABEL:
        lmk2_label_out_put = output_tensors_[out_level].data.virAddr;
        break;
      case FasterRCNNBranchOutType::LMKS2_OFFSET:
        lmk2_offset_out_put = output_tensors_[out_level].data.virAddr;
        break;
      case FasterRCNNBranchOutType::KPS_LABEL:
        kps_label_out_put = output_tensors_[out_level].data.virAddr;
        break;
      case FasterRCNNBranchOutType::KPS_OFFSET:
        kps_offset_out_put = output_tensors_[out_level].data.virAddr;
        break;
      default:
        break;
    }
  }

  for (int out_level = 0; out_level < bpu_model_->output_num; ++out_level) {
    const auto &branch_info = out_level2rcnn_branch_info_[out_level];
    auto out_type = branch_info.type;
    switch (out_type) {
      case FasterRCNNBranchOutType::INVALID:
        break;
      case FasterRCNNBranchOutType::KPS:
        LOGD << "begin GetKps";
        GetKps(det_result.landmarks[branch_info.name],
               output_tensors_[out_level].data.virAddr,
               det_result.boxes[branch_info.box_name]);
        break;
      case FasterRCNNBranchOutType::KPS_LABEL:
        LOGD << "begin GetKps2";
        GetKps2(det_result.landmarks["kps"], kps_label_out_put,
                kps_offset_out_put, det_result.boxes[branch_info.box_name]);
        break;
      case FasterRCNNBranchOutType::MASK:
        LOGD << "begin GetMask";
        GetMask(det_result.segmentations[branch_info.name],
                output_tensors_[out_level].data.virAddr,
                det_result.boxes[branch_info.box_name]);
        break;
      case FasterRCNNBranchOutType::REID:
        LOGD << "begin GetReid";
        GetReid(det_result.features[branch_info.name],
                output_tensors_[out_level].data.virAddr,
                det_result.boxes[branch_info.box_name]);
        break;
      case FasterRCNNBranchOutType::LMKS:
        LOGD << "begin GetLMKS";
        GetLMKS(det_result.landmarks["landmark"],
                output_tensors_[out_level].data.virAddr,
                det_result.boxes[branch_info.box_name]);
        break;
      case FasterRCNNBranchOutType::LMKS2_LABEL:
        LOGD << "begin GetLMKS2";
        GetLMKS2(det_result.landmarks["landmark2"], lmk2_label_out_put,
                 lmk2_offset_out_put, det_result.boxes[branch_info.box_name]);
        break;
      case FasterRCNNBranchOutType::LMKS1:
        LOGD << "begin GetLMKS1";
        GetLMKS1(det_result.landmarks["landmark1"],
                 output_tensors_[out_level].data.virAddr,
                 det_result.boxes[branch_info.box_name]);
        break;
      case FasterRCNNBranchOutType::POSE_3D:
        LOGD << "begin GetPose";
        GetPose(det_result.poses[branch_info.name],
                output_tensors_[out_level].data.virAddr,
                det_result.boxes[branch_info.box_name]);
        break;
      case FasterRCNNBranchOutType::PLATE_COLOR:
        LOGD << "begin GetPlateColor";
        GetPlateColor(&(det_result.attributes[branch_info.name]),
                      output_tensors_[out_level].data.virAddr,
                      det_result.boxes[branch_info.box_name]);
        break;
      case FasterRCNNBranchOutType::PLATE_ROW:
        LOGD << "begin GetPlateRow";
        GetPlateRow(&(det_result.attributes[branch_info.name]),
                    output_tensors_[out_level].data.virAddr,
                    det_result.boxes[branch_info.box_name]);
        break;
      default:
        break;
    }
  }
}

void FasterRCNNImp::GetRppRects(
    std::map<std::string, std::vector<BBox>> &boxes,
    int index, const std::string &name,
    const std::unordered_map<int, std::string> &labels) {
  BPU_RPP_BBOX rpp_bbox;
  int ret = HB_BPU_parseRPPResult(bpu_model_,
                                  output_tensors_.data(),
                                  index,
                                  &rpp_bbox);
  if (ret != 0) {
    LOGE << "here parse model output failed.";
    return;
  }
  int nBox = rpp_bbox.bbox_num;
  HOBOT_CHECK(rpp_bbox.result_type == BPU_RPP_BBOX::bbox_type_f32);

  auto box_ptr = rpp_bbox.bbox_ptr_f32;
  LOGD << "rpp box num: " << nBox;

  for (int i = 0; i < nBox; ++i) {
    hobot::vision::BBox box;
    box.x1 = box_ptr[i].left;
    box.y1 = box_ptr[i].top;
    box.x2 = box_ptr[i].right;
    box.y2 = box_ptr[i].bottom;
    box.score = box_ptr[i].score;
    int type = box_ptr[i].class_label;
    if (labels.find(type) != labels.end()) {
      box.category_name = labels.at(type);
    } else {
      box.category_name = name;
    }
    boxes[box.category_name].push_back(std::move(box));
  }
}

void FasterRCNNImp::GetKps(std::vector<Landmarks> &kpss,
                           void* output,
                           const std::vector<BBox> &body_boxes) {
  int32_t *kps_feature = reinterpret_cast<int32_t *>(output);

  float pos_distance = kps_pos_distance_ * kps_feat_width_;
  size_t body_box_num = body_boxes.size();
  int feature_size = aligned_kps_dim[1] *
                     aligned_kps_dim[2] *
                     aligned_kps_dim[3];
  int h_stride = aligned_kps_dim[2] * aligned_kps_dim[3];
  int w_stride = aligned_kps_dim[3];

  for (size_t box_id = 0; box_id < body_box_num; ++box_id) {
    const auto &body_box = body_boxes[box_id];
    float x1 = body_box.x1;
    float y1 = body_box.y1;
    float x2 = body_box.x2;
    float y2 = body_box.y2;
    float w = x2 - x1 + 1;
    float h = y2 - y1 + 1;

    float scale_x = kps_feat_width_ / w;
    float scale_y = kps_feat_height_ / h;

    Landmarks skeleton;
    skeleton.values.resize(kps_points_number_);

    auto *mxnet_out_for_one_point_begin = kps_feature + feature_size * box_id;
    for (int kps_id = 0; kps_id < kps_points_number_; ++kps_id) {
      // find the best position
      int max_w = 0;
      int max_h = 0;
      int max_score_before_shift = mxnet_out_for_one_point_begin[kps_id];
      int32_t *mxnet_out_for_one_point = nullptr;
      for (int hh = 0; hh < kps_feat_height_; ++hh) {
        for (int ww = 0; ww < kps_feat_width_; ++ww) {
          mxnet_out_for_one_point =
              mxnet_out_for_one_point_begin + hh * h_stride + ww * w_stride;
          if (mxnet_out_for_one_point[kps_id] > max_score_before_shift) {
            max_w = ww;
            max_h = hh;
            max_score_before_shift = mxnet_out_for_one_point[kps_id];
          }
        }
      }

      float max_score =
          GetFloatByInt(max_score_before_shift, kps_shifts_[kps_id]);

      // get delta
      mxnet_out_for_one_point =
          mxnet_out_for_one_point_begin + max_h * h_stride + max_w * w_stride;
      const auto x_delta =
          mxnet_out_for_one_point[2 * kps_id + kps_points_number_];
      const auto x_shift = kps_shifts_[2 * kps_id + kps_points_number_];
      float fp_delta_x = GetFloatByInt(x_delta, x_shift) * pos_distance;

      const auto y_delta =
          mxnet_out_for_one_point[2 * kps_id + kps_points_number_ + 1];
      const auto y_shift = kps_shifts_[2 * kps_id + kps_points_number_ + 1];
      float fp_delta_y = GetFloatByInt(y_delta, y_shift) * pos_distance;

      Point point;
      point.x =
          (max_w + fp_delta_x + 0.46875 + kps_anchor_param_) / scale_x + x1;
      point.y =
          (max_h + fp_delta_y + 0.46875 + kps_anchor_param_) / scale_y + y1;
      point.score = SigMoid(max_score);
      skeleton.values[kps_id] = point;
    }
    kpss.push_back(std::move(skeleton));
  }
}

void FasterRCNNImp::GetKps2(
    std::vector<hobot::vision::Landmarks> &kpss,
    void* kps_label_output, void* kps_offset_output,
    const std::vector<hobot::vision::BBox> &body_boxes) {
  int32_t *label_feature = reinterpret_cast<int32_t *>(kps_label_output);
  int32_t *offset_feature = reinterpret_cast<int32_t *>(kps_offset_output);

  int input_height = kps_feat_height_ * kps_feat_stride_;
  int input_width = kps_feat_width_ * kps_feat_stride_;
  float base_center = (kps_feat_stride_ - 1) / 2.0;

  int label_feature_size = aligned_kps_label_dim[1] * aligned_kps_label_dim[2] *
                           aligned_kps_label_dim[3];
  int label_h_stride = aligned_kps_label_dim[2] * aligned_kps_label_dim[3];
  int label_w_stride = aligned_kps_label_dim[3];

  int offset_feature_size = aligned_kps_offset_dim[1] *
                            aligned_kps_offset_dim[2] *
                            aligned_kps_offset_dim[3];
  int offset_h_stride = aligned_kps_offset_dim[2] * aligned_kps_offset_dim[3];
  int offset_w_stride = aligned_kps_offset_dim[3];

  size_t body_box_num = body_boxes.size();

  for (size_t box_id = 0; box_id < body_box_num; ++box_id) {
    const auto &body_box = body_boxes[box_id];
    float x1 = body_box.x1;
    float y1 = body_box.y1;
    float x2 = body_box.x2;
    float y2 = body_box.y2;
    float w = x2 - x1 + 1;
    float h = y2 - y1 + 1;

    float scale_x = w / input_width;
    float scale_y = h / input_height;
    float scale_pos_x = kps_pos_distance_ * scale_x;
    float scale_pos_y = kps_pos_distance_ * scale_y;

    Landmarks skeleton;
    skeleton.values.resize(kps_points_number_);
    auto *label_feature_begin = label_feature + label_feature_size * box_id;

    for (int kps_id = 0; kps_id < kps_points_number_; ++kps_id) {
      // find the best position
      int max_w = 0;
      int max_h = 0;
      int max_score_before_shift = label_feature_begin[kps_id];
      int32_t *mxnet_out_for_one_point = nullptr;
      for (int hh = 0; hh < kps_feat_height_; ++hh) {
        for (int ww = 0; ww < kps_feat_width_; ++ww) {
          mxnet_out_for_one_point =
              label_feature_begin + hh * label_h_stride + ww * label_w_stride;
          if (mxnet_out_for_one_point[kps_id] > max_score_before_shift) {
            max_w = ww;
            max_h = hh;
            max_score_before_shift = mxnet_out_for_one_point[kps_id];
          }
        }
      }
      float max_score = GetFloatByInt(max_score_before_shift, kps_label_shift_);

      float base_x = (max_w * kps_feat_stride_ + base_center) * scale_x + x1;
      float base_y = (max_h * kps_feat_stride_ + base_center) * scale_y + y1;

      // get delta
      int32_t *offset_feature_begin =
          offset_feature + offset_feature_size * box_id;
      mxnet_out_for_one_point = offset_feature_begin + max_h * offset_h_stride +
                                max_w * offset_w_stride;

      const auto x_delta = mxnet_out_for_one_point[2 * kps_id];
      float fp_delta_x = GetFloatByInt(x_delta, kps_offset_shift_);

      const auto y_delta = mxnet_out_for_one_point[2 * kps_id + 1];
      float fp_delta_y = GetFloatByInt(y_delta, kps_offset_shift_);

      Point point;
      point.x = base_x + fp_delta_x * scale_pos_x;
      point.y = base_y + fp_delta_y * scale_pos_y;
      point.score = SigMoid(max_score);
      skeleton.values[kps_id] = point;
    }
    kpss.push_back(std::move(skeleton));
  }
}

void FasterRCNNImp::GetMask(std::vector<Segmentation> &masks,
                            void* output,
                            const std::vector<BBox> &body_boxes) {
  size_t body_box_num = body_boxes.size();
  int32_t *mask_feature = reinterpret_cast<int32_t *>(output);

  int feature_size =
      aligned_mask_dim[1] * aligned_mask_dim[2] * aligned_mask_dim[3];

  int h_stride = aligned_mask_dim[2] * aligned_mask_dim[3];
  int w_stride = aligned_mask_dim[3];


  for (size_t box_id = 0; box_id < body_box_num; ++box_id) {
    int32_t *mask_feature_begin = mask_feature + feature_size * box_id;
    int32_t *mxnet_out_for_one_box = nullptr;

    float fp_for_this_mask;
    Segmentation mask;
    for (int hh = 0; hh < aligned_mask_dim[1]; ++hh) {
      for (int ww = 0; ww < aligned_mask_dim[2]; ++ww) {
        mxnet_out_for_one_box =
            mask_feature_begin + hh * h_stride + ww * w_stride;
        fp_for_this_mask = GetFloatByInt(mxnet_out_for_one_box[0], mask_shift_);
        mask.values.push_back(fp_for_this_mask);
      }
    }
    mask.height = aligned_mask_dim[1];
    mask.width = aligned_mask_dim[2];
    masks.push_back(std::move(mask));
  }
}

void FasterRCNNImp::GetReid(std::vector<Feature> &reids,
                            void* output,
                            const std::vector<BBox> &body_boxes) {
  size_t body_box_num = body_boxes.size();
  int32_t *reid_feature = reinterpret_cast<int32_t *>(output);

  int feature_size =
      aligned_reid_dim[1] * aligned_reid_dim[2] * aligned_reid_dim[3];

  for (size_t box_id = 0; box_id < body_box_num; ++box_id) {
    int32_t *mxnet_out_for_one_box = reid_feature + feature_size * box_id;
    float fp_for_this_reid;
    Feature reid;
    for (int32_t reid_i = 0; reid_i < aligned_reid_dim[3]; ++reid_i) {
      fp_for_this_reid =
          GetFloatByInt(mxnet_out_for_one_box[reid_i], reid_shift_);
      reid.values.push_back(fp_for_this_reid);
    }
    // l2norm
    l2_norm(reid);
    reids.push_back(std::move(reid));
  }
}

void FasterRCNNImp::GetLMKS(
    std::vector<hobot::vision::Landmarks> &landmarks, void* output,
    const std::vector<hobot::vision::BBox> &face_boxes) {
  int32_t *lmk_feature = reinterpret_cast<int32_t *>(output);

  float pos_distance = lmk_pos_distance_ * lmk_feat_width_;
  size_t face_box_num = face_boxes.size();
  int feature_size =
      aligned_lmks_dim[1] * aligned_lmks_dim[2] * aligned_lmks_dim[3];
  int h_stride = aligned_lmks_dim[2] * aligned_lmks_dim[3];
  int w_stride = aligned_lmks_dim[3];


  for (size_t box_id = 0; box_id < face_box_num; ++box_id) {
    const auto &face_box = face_boxes[box_id];
    float x1 = face_box.x1;
    float y1 = face_box.y1;
    float x2 = face_box.x2;
    float y2 = face_box.y2;
    float w = x2 - x1 + 1;
    float h = y2 - y1 + 1;

    float scale_x = lmk_feat_width_ / w;
    float scale_y = lmk_feat_height_ / h;

    Landmarks landmark;
    landmark.values.resize(lmk_points_number_);

    for (int lmk_id = 0; lmk_id < lmk_points_number_; ++lmk_id) {
      int32_t *lmk_feature_begin = lmk_feature + feature_size * box_id;
      // find the best position
      int max_w = 0;
      int max_h = 0;
      int max_score_before_shift = lmk_feature_begin[lmk_id];
      int32_t *mxnet_out_for_one_point = nullptr;
      for (int hh = 0; hh < lmk_feat_height_; ++hh) {
        for (int ww = 0; ww < lmk_feat_width_; ++ww) {
          mxnet_out_for_one_point =
              lmk_feature_begin + hh * h_stride + ww * w_stride;
          if (mxnet_out_for_one_point[lmk_id] > max_score_before_shift) {
            max_w = ww;
            max_h = hh;
            max_score_before_shift = mxnet_out_for_one_point[lmk_id];
          }
        }
      }
      float max_score = GetFloatByInt(max_score_before_shift, lmks_shift_);

      // get delta
      mxnet_out_for_one_point =
          lmk_feature_begin + max_h * h_stride + max_w * w_stride;
      const auto x_delta =
          mxnet_out_for_one_point[2 * lmk_id + lmk_points_number_];
      const auto x_shift = kps_shifts_[2 * lmk_id + lmk_points_number_];
      float fp_delta_x = GetFloatByInt(x_delta, x_shift) * pos_distance;
      const auto y_delta =
          mxnet_out_for_one_point[2 * lmk_id + lmk_points_number_ + 1];
      const auto y_shift = kps_shifts_[2 * lmk_id + lmk_points_number_ + 1];
      float fp_delta_y = GetFloatByInt(y_delta, y_shift) * pos_distance;

      Point point;
      point.x =
          (max_w + fp_delta_x + 0.46875 + lmk_anchor_param_) / scale_x + x1;
      point.y =
          (max_h + fp_delta_y + 0.46875 + lmk_anchor_param_) / scale_y + y1;
      point.score = max_score;
      landmark.values[lmk_id] = point;
    }
    landmarks.push_back(std::move(landmark));
  }
}

void FasterRCNNImp::GetLMKS2(std::vector<Landmarks> &landmarks,
                             void* lmks2_label_output,
                             void* lmks2_offset_output,
                             const std::vector<BBox> &face_boxes) {
  int32_t *label_feature = reinterpret_cast<int32_t *>(lmks2_label_output);
  int32_t *offset_feature = reinterpret_cast<int32_t *>(lmks2_offset_output);

  int input_height = lmk_feat_height_ * lmk_feat_stride_;
  int input_width = lmk_feat_width_ * lmk_feat_stride_;
  float base_center = (lmk_feat_stride_ - 1) / 2.0;
  size_t face_box_num = face_boxes.size();

  int label_feature_size = aligned_lmks2_label_dim[1] *
                           aligned_lmks2_label_dim[2] *
                           aligned_lmks2_label_dim[3];
  int label_h_stride = aligned_lmks2_label_dim[2] * aligned_lmks2_label_dim[3];
  int label_w_stride = aligned_lmks2_label_dim[3];
  int offset_feature_size = aligned_lmks2_offset_dim[1] *
                            aligned_lmks2_offset_dim[2] *
                            aligned_lmks2_offset_dim[3];
  int offset_h_stride =
      aligned_lmks2_offset_dim[2] * aligned_lmks2_offset_dim[3];
  int offset_w_stride = aligned_lmks2_offset_dim[3];

  for (size_t box_id = 0; box_id < face_box_num; ++box_id) {
    const auto &face_box = face_boxes[box_id];
    float x1 = face_box.x1;
    float y1 = face_box.y1;
    float x2 = face_box.x2;
    float y2 = face_box.y2;
    float w = x2 - x1 + 1;
    float h = y2 - y1 + 1;

    assert(input_width != 0 && input_height != 0);
    float scale_x = w / input_width;
    float scale_y = h / input_height;
    float scale_pos_x = lmk_pos_distance_ * scale_x;
    float scale_pos_y = lmk_pos_distance_ * scale_y;

    Landmarks landmark;
    landmark.values.resize(lmk_points_number_);
    int32_t *label_feature_begin = label_feature + label_feature_size * box_id;
    int32_t *offset_feature_begin =
        offset_feature + offset_feature_size * box_id;
    for (int kps_id = 0; kps_id < lmk_points_number_; ++kps_id) {
      // find the best position
      int max_w = 0;
      int max_h = 0;
      int max_score_before_shift = label_feature_begin[kps_id];
      int32_t *mxnet_out_for_one_point = nullptr;
      for (int hh = 0; hh < lmk_feat_height_; ++hh) {
        for (int ww = 0; ww < lmk_feat_width_; ++ww) {
          mxnet_out_for_one_point =
              label_feature_begin + hh * label_h_stride + ww * label_w_stride;
          if (mxnet_out_for_one_point[kps_id] > max_score_before_shift) {
            max_w = ww;
            max_h = hh;
            max_score_before_shift = mxnet_out_for_one_point[kps_id];
          }
        }
      }
      float max_score = GetFloatByInt(max_score_before_shift,
                                      lmks2_label_shift_);
      float base_x = (max_w * lmk_feat_stride_ + base_center) * scale_x + x1;
      float base_y = (max_h * lmk_feat_stride_ + base_center) * scale_y + y1;

      // get delta
      mxnet_out_for_one_point = offset_feature_begin + max_h * offset_h_stride +
                                max_w * offset_w_stride;
      auto x_delta = mxnet_out_for_one_point[2 * kps_id];
      float fp_delta_x = GetFloatByInt(x_delta, lmks2_offset_shift_);
      auto y_delta = mxnet_out_for_one_point[2 * kps_id + 1];
      float fp_delta_y = GetFloatByInt(y_delta, lmks2_offset_shift_);
      Point point;
      point.x = base_x + fp_delta_x * scale_pos_x;
      point.y = base_y + fp_delta_y * scale_pos_y;
      point.score = SigMoid(max_score);
      landmark.values[kps_id] = point;
    }
    landmarks.push_back(std::move(landmark));
  }
}

void FasterRCNNImp::GetLMKS1(std::vector<Landmarks> &landmarks,
                             void* output,
                             const std::vector<BBox> &face_boxes) {
  size_t face_box_num = face_boxes.size();
  int32_t *lmks1_feature =
      reinterpret_cast<int32_t *>(output);

  int feature_size =
      aligned_lmks1_dim[1] * aligned_lmks1_dim[2] * aligned_lmks1_dim[3];


  for (size_t box_id = 0; box_id < face_box_num; ++box_id) {
    const auto &face_box = face_boxes[box_id];
    float x1 = face_box.x1;
    float y1 = face_box.y1;
    float x2 = face_box.x2;
    float y2 = face_box.y2;
    float w = x2 - x1 + 1;
    float h = y2 - y1 + 1;

    int32_t *mxnet_out_for_this_box = lmks1_feature + feature_size * box_id;

    hobot::vision::Landmarks landmark;
    for (int i = 0; i < lmk_points_number_; ++i) {
      float x =
          GetFloatByInt(mxnet_out_for_this_box[2 * i], lmks1_shift_) * w + x1;
      float y =
          GetFloatByInt(mxnet_out_for_this_box[2 * i + 1], lmks1_shift_) * h +
          y1;
      landmark.values.push_back(Point(x, y));
    }
    landmarks.push_back(std::move(landmark));
  }
}

void FasterRCNNImp::GetPose(std::vector<Pose3D> &face_pose,
                            void* output,
                            const std::vector<BBox> &face_boxes) {
  size_t face_box_num = face_boxes.size();
  int32_t *pose_feature = reinterpret_cast<int32_t *>(output);

  int feature_size = aligned_face_pose_dim[1] * aligned_face_pose_dim[2] *
                     aligned_face_pose_dim[3];

  for (size_t box_id = 0; box_id < face_box_num; ++box_id) {
    int32_t *mxnet_out_for_one_box = pose_feature + feature_size * box_id;
    hobot::vision::Pose3D pose;
    pose.yaw = GetFloatByInt(mxnet_out_for_one_box[0], face_pose_shift_) * 90.0;
    LOGD << "pose.yaw: " << pose.yaw;
    pose.pitch =
        GetFloatByInt(mxnet_out_for_one_box[1], face_pose_shift_) * 90.0;
    LOGD << "pose.pitch: " << pose.pitch;
    pose.roll =
        GetFloatByInt(mxnet_out_for_one_box[2], face_pose_shift_) * 90.0;
    LOGD << "pose.roll: " << pose.roll;
    face_pose.push_back(pose);
  }
}

void FasterRCNNImp::GetPlateColor(
    std::vector<hobot::vision::Attribute<int>> *plates_color,
    void* output,
    const std::vector<hobot::vision::BBox> &plate_boxes) {
  size_t plate_box_num = plate_boxes.size();
  int32_t *plate_color_feature = reinterpret_cast<int32_t *>(output);

  int feature_size = aligned_plate_color_dim[1] * aligned_plate_color_dim[2] *
                     aligned_plate_color_dim[3];

  LOGD << "plate color: ";
  for (size_t box_id = 0; box_id < plate_box_num; ++box_id) {
    int32_t *mxnet_out_for_one_box =
        plate_color_feature + feature_size * box_id;

    hobot::vision::Attribute<int> one_plate_color;
    int max_index = 0;
    float max_score = -1000;
    for (int32_t color_index = 0; color_index < plate_color_num_;
         ++color_index) {
      float color_score =
          GetFloatByInt(mxnet_out_for_one_box[color_index], plate_color_shift_);
      if (color_score > max_score) {
        max_score = color_score;
        max_index = color_index;
      }
    }
    one_plate_color.value = max_index;
    one_plate_color.score = max_score;
    LOGD << "value: " << one_plate_color.value
         << ", score: " << one_plate_color.score << "\n";
    plates_color->push_back(one_plate_color);
  }
}

void FasterRCNNImp::GetPlateRow(
    std::vector<hobot::vision::Attribute<int>> *plates_row,
    void* output,
    const std::vector<hobot::vision::BBox> &plate_boxes) {
  size_t plate_box_num = plate_boxes.size();
  int32_t *plate_row_feature = reinterpret_cast<int32_t *>(output);

  int feature_size = aligned_plate_row_dim[1] * aligned_plate_row_dim[2] *
                     aligned_plate_row_dim[3];


  LOGD << "plate row: ";
  for (size_t box_id = 0; box_id < plate_box_num; ++box_id) {
    int32_t *mxnet_out_for_one_box = plate_row_feature + feature_size * box_id;
    hobot::vision::Attribute<int> one_plate_row;

    int max_index = 0;
    float max_score = -1000;
    for (int32_t row_index = 0; row_index < plate_row_num_; ++row_index) {
      float row_score =
          GetFloatByInt(mxnet_out_for_one_box[row_index], plate_row_shift_);
      if (row_score > max_score) {
        max_score = row_score;
        max_index = row_index;
      }
    }
    one_plate_row.value = max_index;
    one_plate_row.score = max_score;
    LOGD << "value: " << one_plate_row.value
         << ", score: " << one_plate_row.score << "\n";
    plates_row->push_back(one_plate_row);
  }
}

int FasterRCNNImp::RunModel(
    uint8_t *img_data, int data_length,
    int image_width, int image_height,
    BPU_DATA_TYPE_E data_type) {
  {
    RUN_PROCESS_TIME_PROFILER("Run FasterRCNNImp PrepareInputTensor");
    RUN_FPS_PROFILER("Run FasterRCNNImp PrepareInputTensor");
    // 1. prepare input
    PrepareInputTensor(img_data, data_length,
                       image_width, image_height, data_type);
  }

  // 2. prepare output tensor
  PrepareOutputTensor();

  // 3. run
  BPU_RUN_CTRL_S run_ctrl_s;
  run_ctrl_s.core_id = core_id_;
  run_ctrl_s.resize_type = 1;
  BPU_TASK_HANDLE task_handle;
  int ret = 0;
  {
    RUN_PROCESS_TIME_PROFILER("Run FasterRCNNImp HB_BPU_runModel");
    RUN_FPS_PROFILER("Run FasterRCNNImp HB_BPU_runModel");
    ret = HB_BPU_runModel(bpu_model_,
                              input_tensors_.data(),
                              bpu_model_->input_num,
                              output_tensors_.data(),
                              bpu_model_->output_num,
                              &run_ctrl_s,
                              true,
                              &task_handle);
  }

  if (ret != 0) {
    LOGE << "bpu run model failed, " << HB_BPU_getErrorName(ret);
    // release input
    ReleaseInputTensor(input_tensors_);
    return ret;
  }
  {
    RUN_PROCESS_TIME_PROFILER("Run FasterRCNNImp HB_BPU_waitModelDone");
    RUN_FPS_PROFILER("Run FasterRCNNImp HB_BPU_waitModelDone");
  }
  // 4. release input
  ReleaseInputTensor(input_tensors_);
  return 0;
}

int FasterRCNNImp::RunModel(uint8_t *img_data,
                            int data_length,
                            BPU_DATA_TYPE_E data_type) {
  // 1. prepare input
  PrepareInputTensor(img_data, data_length, data_type);

  // 2. prepare output tensor
  PrepareOutputTensor();

  // 3. run
  BPU_RUN_CTRL_S run_ctrl_s;
  run_ctrl_s.core_id = core_id_;
  BPU_TASK_HANDLE task_handle;
  int ret = HB_BPU_runModel(bpu_model_,
                            input_tensors_.data(),
                            bpu_model_->input_num,
                            output_tensors_.data(),
                            bpu_model_->output_num,
                            &run_ctrl_s,
                            true,
                            &task_handle);

  if (ret != 0) {
    LOGE << "bpu run model failed, " << HB_BPU_getErrorName(ret);
    // release input
    ReleaseInputTensor(input_tensors_);
    return ret;
  }
  // 4. release input
  ReleaseInputTensor(input_tensors_);
  return 0;
}

int FasterRCNNImp::RunModelFromPym(void* pyramid,
                                   int pym_layer,
                                   BPU_DATA_TYPE_E data_type) {
  auto pyramid_image =
      reinterpret_cast<PymImageFrame*>(pyramid);
  // 1. prepare input
  input_tensors_.resize(bpu_model_->input_num);
  for (int i = 0; i < bpu_model_->input_num; i++) {
    BPU_TENSOR_S &tensor = input_tensors_[i];
    BPU_MODEL_NODE_S &node = bpu_model_->inputs[i];
    tensor.data_type = data_type;
    tensor.data_shape.layout = node.shape.layout;
    tensor.aligned_shape.layout = node.shape.layout;

    int h_idx, w_idx, c_idx;
    HB_BPU_getHWCIndex(tensor.data_type,
                       &tensor.data_shape.layout,
                       &h_idx, &w_idx, &c_idx);

    LOGD << "node data_type: " << node.data_type;
    int node_h_idx, node_w_idx, node_c_idx;
    HB_BPU_getHWCIndex(node.data_type,
                       &node.shape.layout,
                       &node_h_idx, &node_w_idx, &node_c_idx);
    tensor.data_shape.ndim = 4;
    tensor.data_shape.d[0] = 1;
    tensor.data_shape.d[h_idx] = node.shape.d[node_h_idx];
    tensor.data_shape.d[w_idx] = node.shape.d[node_w_idx];
    tensor.data_shape.d[c_idx] = node.shape.d[node_c_idx];
    tensor.aligned_shape.ndim = 4;
    tensor.aligned_shape.d[0] = 1;
    tensor.aligned_shape.d[h_idx] = node.aligned_shape.d[node_h_idx];
    tensor.aligned_shape.d[w_idx] = node.aligned_shape.d[node_w_idx];
    tensor.aligned_shape.d[c_idx] = node.aligned_shape.d[node_c_idx];
    LOGD << "input_tensor.data_shape.d[0]: " << tensor.data_shape.d[0] << ", "
         << "input_tensor.data_shape.d[1]: " << tensor.data_shape.d[1] << ", "
         << "input_tensor.data_shape.d[2]: " << tensor.data_shape.d[2] << ", "
         << "input_tensor.data_shape.d[3]: " << tensor.data_shape.d[3] << ", "
         << "input_tensor.data_shape.layout: " << tensor.data_shape.layout;

    int image_height = tensor.data_shape.d[h_idx];
    int image_width = tensor.data_shape.d[w_idx];
    int image_channel = tensor.data_shape.d[c_idx];
    int stride = tensor.aligned_shape.d[w_idx];
    LOGD << "image_height: " << image_height << ", "
         << "image_width: " << image_width << ", "
         << "image channel: " << image_channel << ", "
         << "stride: " << stride;

    switch (data_type) {
      case BPU_TYPE_IMG_NV12_SEPARATE: {
    #ifdef X2
        int y_length = image_height * stride;
        int uv_length = image_height / 2 * stride;
        HB_SYS_bpuMemAlloc("in_data0", y_length, true, &tensor.data);
        HB_SYS_bpuMemAlloc("in_data1", uv_length, true, &tensor.data_ext);
        uint8_t* y_data = reinterpret_cast<uint8_t *>(
            pyramid_image->img.down_scale[pym_layer].y_vaddr);
        // Copy y data to data0
        uint8_t *y = reinterpret_cast<uint8_t *>(tensor.data.virAddr);
        for (int h = 0; h < image_height; ++h) {
          auto *raw = y + h * stride;
          memcpy(raw, y_data, image_width);
          y_data += image_width;
        }
        HB_SYS_flushMemCache(&tensor.data, HB_SYS_MEM_CACHE_CLEAN);

        // Copy uv data to data_ext
        uint8_t* uv_data = reinterpret_cast<uint8_t *>(
            pyramid_image->img.down_scale[pym_layer].c_vaddr);
        uint8_t *uv = reinterpret_cast<uint8_t *>(tensor.data_ext.virAddr);
        int uv_height = image_height / 2;
        for (int i = 0; i < uv_height; ++i) {
          auto *raw = uv + i * stride;
          memcpy(raw, uv_data, image_width);
          uv_data += image_width;
        }
        HB_SYS_flushMemCache(&tensor.data_ext, HB_SYS_MEM_CACHE_CLEAN);
    #endif
    #ifdef X3
        // Copy y data to data0
        tensor.data.virAddr = reinterpret_cast<void*>(
                                pyramid_image->down_scale[pym_layer].y_vaddr);
        tensor.data.phyAddr = pyramid_image->down_scale[pym_layer].y_paddr;
        tensor.data.memSize =  image_height * stride;
        // HB_SYS_flushMemCache(&tensor.data, HB_SYS_MEM_CACHE_CLEAN);
        // Copy uv data to data_ext
        tensor.data_ext.virAddr = reinterpret_cast<void*>(
            pyramid_image->down_scale[pym_layer].c_vaddr);
        tensor.data_ext.phyAddr = pyramid_image->down_scale[pym_layer].c_paddr;
        tensor.data_ext.memSize =  (image_height + 1) / 2 * stride;
        // HB_SYS_flushMemCache(&tensor.data_ext, HB_SYS_MEM_CACHE_CLEAN);
    #endif
        break;
      }
      default:
        HOBOT_CHECK(0) << "unsupport data_type: " << data_type;
        break;
    }
  }

  // 2. prepare output tensor
  PrepareOutputTensor();

  // 3. run
  BPU_RUN_CTRL_S run_ctrl_s;
  run_ctrl_s.core_id = core_id_;
  BPU_TASK_HANDLE task_handle;
  int ret = HB_BPU_runModel(bpu_model_,
                            input_tensors_.data(),
                            bpu_model_->input_num,
                            output_tensors_.data(),
                            bpu_model_->output_num,
                            &run_ctrl_s,
                            true,
                            &task_handle);
  if (ret != 0) {
    LOGE << "bpu run model failed, " << HB_BPU_getErrorName(ret);
#ifdef X2
    ReleaseInputTensor(input_tensors_);
#endif
    return ret;
  }
#ifdef X2
    ReleaseInputTensor(input_tensors_);
#endif
  return 0;
}

void FasterRCNNImp::PrepareInputTensor(uint8_t *img_data,
                                       int data_length,
                                       BPU_DATA_TYPE_E data_type) {
  input_tensors_.resize(bpu_model_->input_num);
  for (int i = 0; i < bpu_model_->input_num; i++) {
    BPU_TENSOR_S &tensor = input_tensors_[i];
    BPU_MODEL_NODE_S &node = bpu_model_->inputs[i];
    tensor.data_type = data_type;
    tensor.data_shape.layout = node.shape.layout;
    tensor.aligned_shape.layout = node.shape.layout;

    int h_idx, w_idx, c_idx;
    HB_BPU_getHWCIndex(tensor.data_type,
                       &tensor.data_shape.layout,
                       &h_idx, &w_idx, &c_idx);

    LOGD << "node data_type: " << node.data_type;
    int node_h_idx, node_w_idx, node_c_idx;
    HB_BPU_getHWCIndex(node.data_type,
                       &node.shape.layout,
                       &node_h_idx, &node_w_idx, &node_c_idx);
    tensor.data_shape.ndim = 4;
    tensor.data_shape.d[0] = 1;
    tensor.data_shape.d[h_idx] = node.shape.d[node_h_idx];
    tensor.data_shape.d[w_idx] = node.shape.d[node_w_idx];
    tensor.data_shape.d[c_idx] = node.shape.d[node_c_idx];
    tensor.aligned_shape.ndim = 4;
    tensor.aligned_shape.d[0] = 1;
    tensor.aligned_shape.d[h_idx] = node.aligned_shape.d[node_h_idx];
    tensor.aligned_shape.d[w_idx] = node.aligned_shape.d[node_w_idx];
    tensor.aligned_shape.d[c_idx] = node.aligned_shape.d[node_c_idx];
    LOGD << "input_tensor.data_shape.d[0]: " << tensor.data_shape.d[0] << ", "
         << "input_tensor.data_shape.d[1]: " << tensor.data_shape.d[1] << ", "
         << "input_tensor.data_shape.d[2]: " << tensor.data_shape.d[2] << ", "
         << "input_tensor.data_shape.d[3]: " << tensor.data_shape.d[3] << ", "
         << "input_tensor.data_shape.layout: " << tensor.data_shape.layout;

    int image_height = tensor.data_shape.d[h_idx];
    int image_width = tensor.data_shape.d[w_idx];
    int image_channel = tensor.data_shape.d[c_idx];
    LOGD << "image_height: " << image_height << ", "
         << "image_width: " << image_width << ", "
         << "image channel: " << image_channel;

    switch (data_type) {
      case BPU_TYPE_IMG_NV12_SEPARATE: {
        int stride = tensor.aligned_shape.d[w_idx];
        int y_length = image_height * stride;
        int uv_length = image_height / 2 * stride;
        HB_SYS_bpuMemAlloc("in_data0", y_length, true, &tensor.data);
        HB_SYS_bpuMemAlloc("in_data1", uv_length, true, &tensor.data_ext);
        HOBOT_CHECK(image_height*image_width*3/2 == data_length)
            << "Input img length error!";
        // Copy y data to data0
        uint8_t *y = reinterpret_cast<uint8_t *>(tensor.data.virAddr);
        for (int h = 0; h < image_height; ++h) {
          auto *raw = y + h * stride;
          memcpy(raw, img_data, image_width);
          img_data += image_width;
        }
        HB_SYS_flushMemCache(&tensor.data, HB_SYS_MEM_CACHE_CLEAN);

        // Copy uv data to data_ext
        uint8_t *uv = reinterpret_cast<uint8_t *>(tensor.data_ext.virAddr);
        int uv_height = image_height / 2;
        for (int i = 0; i < uv_height; ++i) {
          auto *raw = uv + i * stride;
          memcpy(raw, img_data, image_width);
          img_data += image_width;
        }
        HB_SYS_flushMemCache(&tensor.data_ext, HB_SYS_MEM_CACHE_CLEAN);
        break;
      }
      default:
        HOBOT_CHECK(0) << "unsupport data_type: " << data_type;
        break;
    }
  }
}

int FasterRCNNImp::PrepareInputTensor(
    uint8_t *img_data, int data_length,
    int image_width, int image_height,
    BPU_DATA_TYPE_E data_type) {
  input_tensors_.resize(bpu_model_->input_num);
  for (int i = 0; i < bpu_model_->input_num; i++) {
    BPU_TENSOR_S &tensor = input_tensors_.at(i);
    BPU_MODEL_NODE_S &node = bpu_model_->inputs[i];
    tensor.data_type = data_type;
    tensor.data_shape.layout = node.shape.layout;
    tensor.aligned_shape.layout = node.shape.layout;

    LOGD << "node data_type: " << node.data_type;

    int h_idx, w_idx, c_idx;
    HB_BPU_getHWCIndex(tensor.data_type,
                       &tensor.data_shape.layout,
                       &h_idx, &w_idx, &c_idx);

    tensor.data_shape.ndim = 4;
    tensor.data_shape.d[0] = 1;
    tensor.data_shape.d[h_idx] = image_height;
    tensor.data_shape.d[w_idx] = image_width;
    tensor.data_shape.d[c_idx] = node.shape.d[c_idx];
    tensor.aligned_shape.ndim = 4;
    tensor.aligned_shape.d[0] = 1;
    tensor.aligned_shape.d[h_idx] = image_height;
    tensor.aligned_shape.d[w_idx] = (image_width + 15) & ~15;
    tensor.aligned_shape.d[c_idx] = node.aligned_shape.d[c_idx];
    LOGD << "input_tensor.data_shape.d[0]: " << tensor.data_shape.d[0] << ", "
         << "input_tensor.data_shape.d[1]: " << tensor.data_shape.d[1] << ", "
         << "input_tensor.data_shape.d[2]: " << tensor.data_shape.d[2] << ", "
         << "input_tensor.data_shape.d[3]: " << tensor.data_shape.d[3] << ", "
         << "input_tensor.data_shape.layout: " << tensor.data_shape.layout;

    int image_channel = tensor.data_shape.d[c_idx];
    int stride = tensor.aligned_shape.d[w_idx];
    LOGD << "image_height: " << image_height << ", "
         << "image_width: " << image_width << ", "
         << "image channel: " << image_channel << ", "
         << "stride: " << stride;

    switch (data_type) {
      case BPU_TYPE_IMG_NV12_SEPARATE: {
        int y_length = image_height * stride;
        int uv_length = image_height * stride / 2;
        HB_SYS_bpuMemAlloc("in_data0", y_length, true, &tensor.data);
        HB_SYS_bpuMemAlloc("in_data1", uv_length, true, &tensor.data_ext);
        // Copy y data to data0
        uint8_t *y = reinterpret_cast<uint8_t *>(tensor.data.virAddr);
        for (int h = 0; h < image_height; ++h) {
          auto *raw = y + h * stride;
          memcpy(raw, img_data, image_width);
          img_data += image_width;
        }
        HB_SYS_flushMemCache(&tensor.data, HB_SYS_MEM_CACHE_CLEAN);

        // Copy uv data to data_ext
        uint8_t *uv = reinterpret_cast<uint8_t *>(tensor.data_ext.virAddr);
        int uv_height = image_height / 2;
        for (int i = 0; i < uv_height; ++i) {
          auto *raw = uv + i * stride;
          memcpy(raw, img_data, image_width);
          img_data += image_width;
        }
        HB_SYS_flushMemCache(&tensor.data_ext, HB_SYS_MEM_CACHE_CLEAN);
        break;
      }
      default:
        HOBOT_CHECK(0) << "unsupport data_type: " << data_type;
        break;
    }
  }
  return 0;
}

void FasterRCNNImp::FlushOutputTensor() {
  if (!output_tensors_alloced_) {
    return;
  }
  for (int i = 0; i < bpu_model_->output_num; i++) {
    if (HB_SYS_isMemCachable(&(output_tensors_[i].data))) {
      HB_SYS_flushMemCache(&(output_tensors_[i].data),
                           HB_SYS_MEM_CACHE_INVALIDATE);
    }
  }
}
void FasterRCNNImp::PrepareOutputTensor() {
  if (output_tensors_alloced_) {
    return;
  }
  output_tensors_.resize(bpu_model_->output_num);
  for (int i = 0; i < bpu_model_->output_num; i++) {
    BPU_TENSOR_S &tensor = output_tensors_[i];
    BPU_MODEL_NODE_S &node = bpu_model_->outputs[i];
    tensor.data_type = node.data_type;
    tensor.data_shape = node.shape;
    tensor.aligned_shape = node.aligned_shape;
    int output_size = 1;
    for (int j = 0; j < node.aligned_shape.ndim; j++) {
      output_size *= node.aligned_shape.d[j];
      LOGD << "node.aligned_shape.d[j]:" << node.aligned_shape.d[j];
    }
    if (node.data_type == BPU_TYPE_TENSOR_F32 ||
        node.data_type == BPU_TYPE_TENSOR_S32 ||
        node.data_type == BPU_TYPE_TENSOR_U32) {
      output_size *= 4;
    }
    // output_size需要按16对齐
    output_size = ALIGN_16(output_size);
    LOGD << "output_size: " << output_size;
    HB_SYS_bpuMemAlloc("out_data0", output_size, true, &tensor.data);
  }
  output_tensors_alloced_ = true;
}

void FasterRCNNImp::ReleaseInputTensor(
    std::vector<BPU_TENSOR_S> &input_tensors) {
  for (size_t i = 0; i < input_tensors.size(); i++) {
    BPU_TENSOR_S *tensor = &input_tensors[i];
    switch (tensor->data_type) {
      case BPU_TYPE_IMG_BGRP:
      case BPU_TYPE_IMG_RGBP:
      case BPU_TYPE_IMG_Y:
      case BPU_TYPE_IMG_RGB:
      case BPU_TYPE_IMG_BGR:
      case BPU_TYPE_IMG_YUV444:
      case BPU_TYPE_IMG_YUV_NV12:
      case BPU_TYPE_TENSOR_U8:
      case BPU_TYPE_TENSOR_S8:
      case BPU_TYPE_TENSOR_F32:
      case BPU_TYPE_TENSOR_S32:
      case BPU_TYPE_TENSOR_U32:
        HB_SYS_bpuMemFree(&(tensor->data));
        break;
      case BPU_TYPE_IMG_NV12_SEPARATE:
        HB_SYS_bpuMemFree(&(tensor->data));
        HB_SYS_bpuMemFree(&(tensor->data_ext));
      default:
        break;
    }
  }
}

void FasterRCNNImp::ReleaseOutputTensor() {
  for (size_t i = 0; i < output_tensors_.size(); i++) {
    HB_SYS_bpuMemFree(&output_tensors_[i].data);
  }
}

void FasterRCNNImp::Finalize() {
  if (output_tensors_alloced_) {
    ReleaseOutputTensor();
    output_tensors_alloced_ = false;
  }
  if (bpu_model_) {
    // release model
    HB_BPU_releaseModel(bpu_model_);
  }
}

int FasterRCNNImp::CropPadAndResizeRoi(
    hobot::vision::BBox *norm_box,
    uint8_t *pym_src_y_data, uint8_t *pym_src_uv_data,
    int pym_src_w, int pym_src_h,
    int pym_src_y_size, int pym_src_uv_size,
    int pym_src_y_stride, int pym_src_uv_stride,
    uint8_t **img_data, HobotXStreamImageToolsResizeInfo *resize_info) {

  uint8_t *pym_dst_data = nullptr;
  int pym_dst_size, pym_dst_w, pym_dst_h, pym_dst_y_stride, pym_dst_uv_stride;
  const uint8_t *input_yuv_data[3] = {pym_src_y_data,
                                      pym_src_uv_data,
                                      nullptr};
  const int input_yuv_size[3] = {pym_src_y_size, pym_src_uv_size, 0};

  int ret = 0;
  {
    RUN_PROCESS_TIME_PROFILER("Run FasterRCNNImp CropYUVImage");
    RUN_FPS_PROFILER("Run FasterRCNNImp CropYUVImage");

    if (norm_box->x2 > pym_src_w) {
      LOGD << "norm_box:" << *norm_box;
      LOGD << "pym_src_w:" << pym_src_w << "  pym_src_h:" << pym_src_h;
      norm_box->x2 = pym_src_w;
    }
    if (norm_box->y2 > pym_src_h) {
      LOGD << "norm_box:" << *norm_box;
      LOGD << "pym_src_w:" << pym_src_w << "  pym_src_h:" << pym_src_h;
      norm_box->y2 = pym_src_h;
    }

    ret = HobotXStreamCropYUVImage(
        input_yuv_data, input_yuv_size, pym_src_w, pym_src_h, pym_src_y_stride,
        pym_src_uv_stride, IMAGE_TOOLS_RAW_YUV_NV12, norm_box->x1, norm_box->y1,
        norm_box->x2 - 1, norm_box->y2 - 1, &pym_dst_data, &pym_dst_size,
        &pym_dst_w, &pym_dst_h, &pym_dst_y_stride, &pym_dst_uv_stride);
    HOBOT_CHECK(ret == 0)
    << "crop img failed"
    << ", pym_src_y_size:" << pym_src_y_size << ", pym_src_w:" << pym_src_w
    << ", pym_src_h:" << pym_src_h << ", pym_src_y_stride:" << pym_src_y_stride
    << ", pym_src_uv_stride:" << pym_src_uv_stride;
  }
  pym_src_y_data = pym_dst_data;
  pym_src_y_size = pym_dst_size;
  pym_src_uv_data = nullptr;
  pym_src_uv_size = 0;
  pym_src_w = pym_dst_w;
  pym_src_h = pym_dst_h;
  pym_src_y_stride = pym_dst_y_stride;
  pym_src_uv_stride = pym_dst_uv_stride;

  RUN_PROCESS_TIME_PROFILER("Run FasterRCNNImp PadYUVImage");
  RUN_FPS_PROFILER("Run FasterRCNNImp PadYUVImage");
  resize_info->src_height_ = pym_dst_h;
  resize_info->src_width_ = pym_dst_w;
  CalcResizeInfo(resize_info);
  const uint8_t padding_value[3] = {0, 128, 128};

  int padding_right =
      resize_info->padding_right_ / resize_info->width_ratio_;
  padding_right &= ~1;
  int padding_bottom =
      resize_info->padding_bottom_ / resize_info->height_ratio_;
  padding_bottom &= ~1;

  ret = HobotXStreamPadImage(
      pym_src_y_data, pym_src_y_size,
      pym_src_w, pym_src_h,
      pym_src_y_stride, pym_src_uv_stride,
      IMAGE_TOOLS_RAW_YUV_NV12,
      0, padding_right, 0, padding_bottom,
      padding_value,
      &pym_dst_data, &pym_dst_size,
      &pym_dst_w, &pym_dst_h,
      &pym_dst_y_stride, &pym_dst_uv_stride);


//  static uint32_t id = 0;
//  std::string output_file_path = std::to_string(id++) + ".yuv";
//  std::ofstream outfile(output_file_path, std::ios::out | std::ios::binary);
//  outfile.write(reinterpret_cast<const char *>(pym_dst_data), pym_dst_size);
//  outfile.close();

  //  free the crop image
  HobotXStreamFreeImage(pym_src_y_data);
  HOBOT_CHECK(ret == 0)
  << "resize img failed, ret: " << ret
  << ", pym_src_y_size:" << pym_src_y_size << ", pym_src_w:" << pym_src_w
  << ", pym_src_h:" << pym_src_h << ", pym_src_y_stride:" << pym_src_y_stride
  << ", pym_src_uv_stride:" << pym_src_uv_stride;

  *img_data = pym_dst_data;
  return 0;
}

void FasterRCNNImp::CalcResizeInfo(
    HobotXStreamImageToolsResizeInfo *resize_info) {
  resize_info->dst_width_ = model_input_width_;
  resize_info->dst_height_ = model_input_height_;
  float width_ratio = static_cast<float>(model_input_width_)
      / resize_info->src_width_;
  float height_ratio = static_cast<float>(model_input_height_)
      / resize_info->src_height_;

  // fix_aspect_ratio
  float ratio = width_ratio;
  if (ratio > height_ratio) {
    ratio = height_ratio;
  }
  resize_info->padding_right_ = model_input_width_ / ratio
      - resize_info->src_width_;
  resize_info->padding_bottom_ = model_input_height_ / ratio
      - resize_info->src_height_;
  resize_info->width_ratio_ = 1;
  resize_info->height_ratio_ = 1;
}

int FasterRCNNImp::GetPymScaleInfo(
    const std::shared_ptr<hobot::vision::PymImageFrame>& pyramid,
    hobot::vision::BBox *norm_box,
    uint8_t* &pym_src_y_data, uint8_t* &pym_src_uv_data,
    int &pym_src_w, int &pym_src_h,
    int &pym_src_y_size, int &pym_src_uv_size,
    int &pym_src_y_stride, int &pym_src_uv_stride, float &scale) {
  auto box_width = norm_box->Width();
  auto box_height = norm_box->Height();
  auto min_cost_scale = 1, minareadiff = INT_MAX;
  int target_area = model_input_width_ * model_input_height_;
  int box_area =  box_height * box_width;
  for (int i = min_cost_scale; i <= 3; i++) {
    int areadiff = std::abs(box_area - target_area);
    if (minareadiff > areadiff) {
      minareadiff = areadiff;
      min_cost_scale = i;
    }
    box_area >>= 2;
  }
  int pyramid_scale = 1 << (min_cost_scale - 1);
  pyramid_scale = (pyramid_scale == 1 ? 0 : pyramid_scale) * 2;
  assert(pyramid_scale >= 0 && pyramid_scale < DOWN_SCALE_MAX);
#ifdef X2
  auto &pyramid_down_scale = pyramid->img.down_scale[pyramid_scale];
  pym_src_y_stride = pyramid_down_scale.step;
  pym_src_uv_stride = pyramid_down_scale.step;
  #endif

#ifdef X3
  auto &pyramid_down_scale = pyramid->down_scale[pyramid_scale];
  pym_src_y_stride = pyramid_down_scale.stride;
  pym_src_uv_stride = pyramid_down_scale.stride;
  #endif

  pym_src_y_data = reinterpret_cast<uint8_t *>(
      pyramid_down_scale.y_vaddr);
  pym_src_uv_data = reinterpret_cast<uint8_t *>(
      pyramid_down_scale.c_vaddr);

  pym_src_w = pyramid_down_scale.width;
  pym_src_h = pyramid_down_scale.height;

  pym_src_y_size = pym_src_y_stride * pym_src_h;
  pym_src_uv_size = pym_src_uv_stride * pym_src_h / 2;

  scale = 1 << (min_cost_scale - 1);
  return 0;
}

int FasterRCNNImp::CropPadAndResizeRoi(
    hobot::vision::BBox *norm_box,
    const std::shared_ptr<hobot::vision::PymImageFrame>& pyramid,
    uint8_t **img_data, HobotXStreamImageToolsResizeInfo *resize_info,
    float *scale) {
  int ret;
  uint8_t *pym_src_y_data , *pym_src_uv_data;
  int pym_src_w, pym_src_h;
  int pym_src_y_size, pym_src_uv_size;

  int pym_src_y_stride, pym_src_uv_stride;

  GetPymScaleInfo(
      pyramid, norm_box,
      pym_src_y_data, pym_src_uv_data,
      pym_src_w, pym_src_h,
      pym_src_y_size, pym_src_uv_size,
      pym_src_y_stride, pym_src_uv_stride, *scale);

  auto scale_box = *norm_box;
  scale_box.x1 /= *scale, scale_box.x2 /= *scale;
  scale_box.y1 /= *scale, scale_box.y2 /= *scale;

  ret = CropPadAndResizeRoi(
      &scale_box,
      pym_src_y_data, pym_src_uv_data,
      pym_src_w, pym_src_h,
      pym_src_y_size, pym_src_uv_size,
      pym_src_y_stride, pym_src_uv_stride,
      img_data, resize_info);
  return ret;
}

}  // namespace faster_rcnn_method
